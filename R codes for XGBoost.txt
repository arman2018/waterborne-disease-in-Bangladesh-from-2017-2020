##XGBoost model
##My working directory 
setwd("D:/arman18/Research/Waterborne study")

##Required libraries
library(readxl)
library(dplyr)
library(xgboost)
library(SHAPforxgboost)
library(caret)
library(rpart)
library(randomForest)
library(ggplot2)
library(MLmetrics)

#Upload data
data<-read_excel("Model.data.AHC.xlsx",sheet="Data")

#selected variables for model
data1<-data[c(5:11,16)]

#data partition
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(data1$y5, p = 0.7, 
                                  list = FALSE,
                                  times = 1)
training_data <-data1[trainIndex, ]
testing_data <-data1[-trainIndex, ]

##XGBoost model for waterborne
dtrain<-xgb.DMatrix(data=as.matrix(training_data),label=training_data$y5)
dtest<-xgb.DMatrix(data=as.matrix(testing_data),label=testing_data$y5)

# XGBoost parameters: This hyper-parameters set is tuned one
params.xgb <- list(booster = "gbtree", 
                   objective = "reg:squarederror", 
                   eta =0.05, #Reduced learning rate 
                   gamma=5, #Increased to prevent overfitting
                   min_child_weight = 10, # Increased to control complexity
                   lambda=1, #Increased for stronger L2 regularization
                   alpha=0.5 #Added L1 regularizatio
                   )
# xgb corss-validation train
xgbcv <- xgb.cv(params =  params.xgb, 
                data = dtrain, 
                nrounds = 1000, 
                nfold = 10, 
                showsd = F,
                prediction = T,
                stratified = T, 
                print_every_n = 1, 
                early_stopping_rounds = 30, 
                maximize = F
)
#' Final training and metrics evaluation on test sets
set.seed(1)
watchlist = list( test = dtest,
                  train = dtrain) 
model.xgb <- xgb.train(params = params.xgb, 
                       data = dtrain, 
                       nrounds =  46,   
                       watchlist = watchlist, 
                       print_every_n = 1, 
                       early_stopping_rounds =30, 
                       maximize = F
)

#prediction in training
pre<-predict(model.xgb,dtrain)
RMSE(training_data$y5,pre)
MAE(training_data$y5,pre)
MAPE(training_data$y5,pre)

#Model prediction
pred<-predict(model.xgb,dtest)
RMSE(testing_data$y5,pred)
MAE(testing_data$y5,pred)
MAPE(testing_data$y5,pred)

#Feature importance using mean |SHAP| values for XGBoost model as best
shap_values <- shap.values(xgb_model = model.xgb, X_train = dtrain)
shap_values$mean_shap_score


# Learning curve
ggplot(data = xgbcv$evaluation_log, aes(x = iter, y = train_rmse_mean, color = "Train")) +
  geom_line() +
  geom_line(data = xgbcv$evaluation_log, aes(x = iter, y = test_rmse_mean, color = "Test")) +
  labs(title = "Learning Curve", x = "Boosting Rounds", y = "RMSE") +
  scale_color_manual(values = c("Train" = "blue", "Test" = "red"))+theme_bw()
